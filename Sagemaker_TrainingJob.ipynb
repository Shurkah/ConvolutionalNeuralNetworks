{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75628005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: blazingtext-2023-08-26-16-40-57-914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 16:40:58 Starting - Starting the training job...\n",
      "2023-08-26 16:41:14 Starting - Preparing the instances for training......\n",
      "2023-08-26 16:41:59 Downloading - Downloading input data...\n",
      "2023-08-26 16:42:55 Training - Training image download completed. Training in progress...\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[08/26/2023 16:43:00 WARNING 139722509731648] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[08/26/2023 16:43:00 WARNING 139722509731648] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[08/26/2023 16:43:00 INFO 139722509731648] nvidia-smi took: 0.025225162506103516 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[08/26/2023 16:43:00 INFO 139722509731648] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[08/26/2023 16:43:00 INFO 139722509731648] Processing /opt/ml/input/data/train/hello_blaze_train . File size: 40.886911392211914 MB\u001b[0m\n",
      "\u001b[34m[08/26/2023 16:43:00 INFO 139722509731648] Processing /opt/ml/input/data/validation/hello_blaze_train . File size: 40.886911392211914 MB\u001b[0m\n",
      "\u001b[34mRead 8M words\u001b[0m\n",
      "\u001b[34mNumber of words:  33240\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0486  Progress: 2.89%  Million Words/sec: 3.87 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0458  Progress: 8.42%  Million Words/sec: 3.76 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0429  Progress: 14.15%  Million Words/sec: 3.80 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0401  Progress: 19.84%  Million Words/sec: 3.80 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0372  Progress: 25.59%  Million Words/sec: 3.81 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0343  Progress: 31.37%  Million Words/sec: 3.83 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0315  Progress: 36.96%  Million Words/sec: 3.81 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0286  Progress: 42.74%  Million Words/sec: 3.82 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0257  Progress: 48.54%  Million Words/sec: 3.83 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0230  Progress: 54.10%  Million Words/sec: 3.82 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0201  Progress: 59.77%  Million Words/sec: 3.82 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0172  Progress: 65.54%  Million Words/sec: 3.82 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0144  Progress: 71.28%  Million Words/sec: 3.82 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0115  Progress: 77.08%  Million Words/sec: 3.83 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0086  Progress: 82.90%  Million Words/sec: 3.83 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0057  Progress: 88.66%  Million Words/sec: 3.84 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0028  Progress: 94.47%  Million Words/sec: 3.84 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: -0.0000  Progress: 100.00%  Million Words/sec: 3.83 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 3.83 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 3.83\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 10.51\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.8624\u001b[0m\n",
      "\u001b[34mNumber of train examples: 426989\u001b[0m\n",
      "\n",
      "2023-08-26 16:43:25 Uploading - Uploading generated training model\u001b[34m#validation_accuracy: 0.8624\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 426989\u001b[0m\n",
      "\n",
      "2023-08-26 16:44:31 Completed - Training job completed\n",
      "Training seconds: 151\n",
      "Billable seconds: 151\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# If you're following along, you'll need to upload these datasets to your own bucket in S3. \n",
    "\n",
    "val_location = 's3://shubuck/nothingworks/hello_blaze_train'\n",
    "train_location = 's3://shubuck/nothingworks/hello_blaze_train'\n",
    "\n",
    "# We use this prefix to help us determine where the output will go. \n",
    "\n",
    "prefix = 's3://shubuck/nothingworks/'\n",
    "\n",
    "# We need to get the location of the container. \n",
    "\n",
    "container = image_uris.retrieve('blazingtext', session.boto_region_name)\n",
    "\n",
    "# Now that we know which container to use, we can construct the estimator object.\n",
    "estim = sagemaker.estimator.Estimator(container, # The image name of the training container\n",
    "                                    role,      # The IAM role to use (our current role in this case)\n",
    "                                    instance_count=1, # The number of instances to use for training\n",
    "                                    instance_type='ml.m5.large', # The type of instance to use for training\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                                                        # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=session) # The current SageMaker session\n",
    "             \n",
    "# These hyperparameters are beyond the scope of this course, but you can research the algoirthm here: \n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html    \n",
    "    \n",
    "estim.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        mode = 'supervised',\n",
    "                        num_round=200)\n",
    "                        \n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n",
    "\n",
    "# The fit method launches the training job. \n",
    "\n",
    "estim.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
